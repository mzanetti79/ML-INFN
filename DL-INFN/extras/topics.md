
# Day One

0 Intro
-----------
* personal intro
* goals of course
* structure of course
* intro deep learning
* biological neurons, circuits
* parts: weights, bias, activation
* connecting them together - "neuron view"
* softmax activation for predictions
* intro to SGD, loss, one-hot (representation), epochs
* the master formula, how it connects to what we have seen
* define linear model in exercise

1 Hidden Layers
-----------
* layers - feedforward example by hand
* MLPs
* layer representations & hidden layers
* linear vs nonlinear components
* what can weights do?
* what can activation functions do?
* relu, sigmoid, tanh
* this is the core of *deep* learning

2 Learning
-----------
* SGD revisited
* learning rate
* loss function and its nonlinearity
* local minima
* backpropagation
* effect of activation function on learning (gradients of the activation function)

3 Learning - Regularization
-----------
* tips for tuning learning rate - learning rate decay
* momentum
* other gradient descent methods
* minibatches and gradient estimates
* overfitting
* 1. regularization with more data - data augmentation
* 2. regularization on weights
* 3. regularization with dropout

4 Hyperparameter review
-----------
* review effects of hyperparameters


# Day Two

5 CNNs
-----------
* training on GPUs
* intro to convolutions
* structure of CNNs
* 1. convolutional layers
* 2. pooling layers
* 3. fully connected layers

6 Visualizing Representations
-----------
* feature map visualization
* preferred stimuli visualization
* deep dream and style transfer
* mapping representations with tSNE

7 Intro to RNNs
-----------
* problems in modelling sequences
* structure of RNNs
* difficulties for training: long-term dependencies
* LSTM (and variants)

8 Generating Sequences
-----------
* variety of RNN models
* machine translation
* captioning
* question answering
* music generation

9 Review and next steps
-----------
* what is deep learning good for?
* what did we not talk about?
* how can you continue learning?

