{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separability and Hidden Layers\n",
    "\n",
    "The goal of this module is to demonstrate how a neural network makes data separable if it isn't already.\n",
    "\n",
    "You will:\n",
    "* become familiar with the basic functioning of a simple deep network\n",
    "* learn to view a network as a composition of functions\n",
    "* explore the concept of \"representation learning\"\n",
    "\n",
    "Much credit goes to [http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) in inspiring these exercises. It is highly recommended reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use our handy decision boundary plot function from last time\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ[:,0] >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y[:,0], alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning non-separable data\n",
    "\n",
    "Let's take a look at a more challenging dataset, one that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(n_samples=1000,\n",
    "                    noise=0.02,\n",
    "                    factor=0.3)\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y[:,0], alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model we build last time on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model0 = Sequential()\n",
    "model0.add(Dense(2, input_dim=2, activation='softmax'))\n",
    "\n",
    "model0.compile(loss='categorical_crossentropy',\n",
    "               optimizer=SGD(lr=0.04),\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model0.fit(X_train, y_train, nb_epoch=20, batch_size=16)\n",
    "result = model0.evaluate(X_test, y_test)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]\n",
    "\n",
    "plot_decision_boundary(model0, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't do nearly as well, which is no surprise. The model isn't **expressive** enough to represent a more complicated decision boundary.\n",
    "\n",
    "- - - \n",
    "### Exercise 1 - the model's strategy\n",
    "\n",
    "Though our model doesn't have agency or a mind or it's own, it is a nice shorthand to talk about what the model is \"trying\" to do in a given situation. Based on the decision boundary plot, can you explain what the model's strategy is and why it gets the accuracy that it does? Try training the model a few times to see what other solutions it finds. Is there a solution that gives the best accuracy?\n",
    "- - -\n",
    "\n",
    "\n",
    "- - -\n",
    "### Exercise 2 - expressiveness and structure\n",
    "\n",
    "Recalling the structure of our model that you already sketched out, can you explain why this model can only make a linear decision boundary?\n",
    "- - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a hidden layer\n",
    "\n",
    "We can make our model more expressive by adding a hidden layer. As we will see, this expands the expressiveness and representational capacity of the model, giving it enough power to separate the data.\n",
    "\n",
    "- - -\n",
    "### Exercise 3 - add a hidden layer\n",
    "\n",
    "Below is the model specification we used already. Modify it in place to:\n",
    "\n",
    "1. make the first layer 3 dimensional rather than 2 dimensional\n",
    "2. add a \"relu\" `Activation` layer and a `Dense` layer that feeds into the last softmax layer. \n",
    "\n",
    "Hint: the `Dense` layer will not need to specify an `input_dim` since Keras will know the input dimension from the previous layer already.\n",
    "- - -\n",
    "\n",
    "<a id=\"compile\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Edit the model below to add new 3-dimensional hidden layer\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(3, input_dim=2)) # Change the hidden layer to be 3 dimensional\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(2))\n",
    "# Add a 'relu' layer and Dense layer with an output of 2 dimensions\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "               optimizer=SGD(lr=0.04),\n",
    "               metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - a more expressive structure\n",
    "\n",
    "From the summary, check that your new model 1) has the same input and output dimension as before and 2) has 17 parameters. \n",
    "\n",
    "Draw this new model. Is it clear where all of the parameters are?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train, nb_epoch=20, batch_size=16)\n",
    "plot_decision_boundary(model1, X, y)\n",
    "\n",
    "result = model1.evaluate(X_test, y_test)\n",
    "print 'Test set loss: ', result[0]\n",
    "print 'Test set accuracy: ', result[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
